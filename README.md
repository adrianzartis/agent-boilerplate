# Agent Boilerplate

This repository provides a minimal FastAPI application showcasing how to build
agents with the [OpenAI Agents SDK](https://github.com/openai/openai-agents).
Two simple agents are orchestrated together: one summarises text and another
creates a follow-up question. All configuration and prompt files live outside
the Python modules so the structure can be easily reused. Use it as a starting
point for your own agents by adding new prompts, schemas and routers.

## Setup

1. Install dependencies:
   ```bash
   pip install -e .
   ```
2. Copy `.env.example` to `.env` and provide your API key:
   ```bash
   cp .env.example .env
   ```
3. Start the server:
   ```bash
   uvicorn src.main:app --reload
   ```

Open `http://localhost:8000/docs` to explore the API. The example
`/api/agent-one/run` endpoint accepts a piece of text and returns both the
summary and a follow-up question generated by a second agent.

The `FollowupOutput` schema also contains a `kind` field indicating if the
question is open ended or can be answered with yes/no.

Pydantic models describing the LLM responses live in `src/schemas.py`. The
`FollowupOutput` model shows how to use `Literal` to constrain values returned
by the language model. FastAPI routers for each agent are defined in their
respective `router.py` modules and are collected by `src/agent.py`. The
application entry point in `src/main.py` simply mounts these routers, configures
basic logging, and adds
some basic middleware.

### Project layout

```
config/            # YAML configuration files
prompts/           # Prompt templates used by each agent
src/
├── agent_one/     # Example agent implementation
│   ├── agent.py   # Defines Agent instances
│   ├── runner.py  # Orchestrates agents together
│   └── router.py  # Exposes FastAPI endpoints
├── schemas.py     # Pydantic models shared across agents
├── agent.py       # Collects routers from all agents
└── main.py        # FastAPI application entry point
```

## Logging

Running the server creates an `app.log` file in the project root. The file is
overwritten on each start and contains debug information including the prompts
sent to the LLMs and their structured responses.

## Extending

The example keeps most components small so you can easily copy and adapt them
when building your own logic. A common workflow for creating a new agent looks
like this:

1. **Create a package** – Copy `src/agent_one` to a new directory such as
   `src/my_agent` and rename the modules inside.
2. **Add prompt templates** – Place new prompt files under
   `prompts/my_agent/` and reference them from `config/config.yaml`.
3. **Define schemas** – Add pydantic models in `src/schemas.py` or a new file
   to describe the expected LLM outputs.
4. **Implement the agents** – Update `my_agent/agent.py` to construct one or
   more `Agent` objects using the new prompts and schemas.
5. **Orchestrate behaviour** – Write a `runner.py` that calls the agents in
   sequence (or in parallel) using `agents.Runner`.
6. **Expose endpoints** – Adjust `router.py` to accept your desired input and
   return the structured output, then register the router in `src/agent.py`.

By repeating these steps you can grow the application with multiple endpoints
and complex chains of agents while keeping configuration and prompts neatly
organized outside the Python source.

